{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "734b0bc2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cuda\n",
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: ./samples\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=[0.5], std=[0.5])\n",
      "           )\n",
      "<__main__.main.<locals>.FacadeDataset object at 0x000001A0ADB8D0A0>\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'total_step' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7236/3851944965.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7236/3851944965.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m300\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m         print(f'Epoch [{epoch}/{num_epochs}], Step [{i+1}/{total_step}],' + \n\u001b[0m\u001b[0;32m    168\u001b[0m         \u001b[1;34mf'd_loss: {d_loss:.8f}, g_loss: {g_loss:.8f}, '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m         f'D(x): {real_score.mean():.2f}, D(G(z)): {fake_score.mean():.2f}')\n",
      "\u001b[1;31mNameError\u001b[0m: name 'total_step' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "#from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def grid_image(tensor_images, size=10, nrow=5):\n",
    "  tensor_images = ((tensor_images + 1) / 2 ).clamp(0, 1)\n",
    "  return torchvision.utils.make_grid(tensor_images[:size], nrow=nrow)\n",
    "\n",
    "def show_image(tensor_images, size=10, nrow=5):\n",
    "  to_pil = transforms.ToPILImage()\n",
    "  grid_img = grid_image(tensor_images)\n",
    "  plt.imshow(to_pil(grid_img), interpolation=\"bicubic\")\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "  def __init__(self, image_size, hidden_size):\n",
    "    super(Discriminator, self).__init__()\n",
    "    self.main = nn.Sequential(\n",
    "        nn.Linear(image_size, hidden_size),\n",
    "        nn.LeakyReLU(0.2),\n",
    "        nn.Linear(hidden_size, hidden_size),\n",
    "        nn.LeakyReLU(0.2),\n",
    "        nn.Linear(hidden_size, 1),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "  def forward(self, x):\n",
    "    return self.main(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "  def __init__(self, latent_size, hidden_size, image_size):\n",
    "    super(Generator, self).__init__()\n",
    "    self.main = nn.Sequential(\n",
    "        nn.Linear(latent_size, hidden_size),\n",
    "        nn.ReLU(0.2),\n",
    "        nn.Linear(hidden_size, hidden_size),\n",
    "        nn.ReLU(0.2),\n",
    "        nn.Linear(hidden_size, image_size),\n",
    "        nn.Tanh()\n",
    "    )\n",
    "  def forward(self, x):\n",
    "    return self.main(x)\n",
    "\n",
    "def binary_cross_entropy(out, target):\n",
    "  return - torch.mean(target * torch.log(out + 1e-10) + (1-target) * torch.log(1-out+1e-10))\n",
    "\n",
    "def main():\n",
    "  # Costum dataset 생성\n",
    "  class FacadeDataset(Dataset):\n",
    "      def __init__(self, path2img, transform=False):\n",
    "          super().__init__()\n",
    "          self.path = path2img\n",
    "          self.img_filenames = [x for x in listdir(self.path)]\n",
    "          self.transform = transform\n",
    "\n",
    "      def __getitem__(self, index):\n",
    "          a = Image.open(join(self.path, self.img_filenames[index])).convert('RGB')\n",
    "        \n",
    "          if self.transform:\n",
    "              a = self.transform(a)\n",
    "\n",
    "      def __len__(self):\n",
    "          return len(self.img_filenames)\n",
    "        \n",
    "        \n",
    "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "  print(f\"using device {device}\")\n",
    "  # Hyper-parameters\n",
    "\n",
    "  latent_size = 64\n",
    "  hidden_size = 256\n",
    "  image_size = 784\n",
    "\n",
    "  num_epochs = 100\n",
    "  batch_size = 100\n",
    "  sample_dir = 'samples'\n",
    "\n",
    "  if not os.path.exists(sample_dir):\n",
    "    os.makedirs(sample_dir)\n",
    "\n",
    "  transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])                              \n",
    "  ])\n",
    "\n",
    "  mnist = torchvision.datasets.MNIST(root='./samples',\n",
    "                                    train=True,\n",
    "                                    transform=transform,\n",
    "                                    download=True)\n",
    "\n",
    "  data_loader = torch.utils.data.DataLoader(dataset=mnist,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True)\n",
    "\n",
    "  fp_op = 'C:/Users/a/big-sleep-new/PyTorch-GAN-master/data/edges2shoes'\n",
    "  train_ds = FacadeDataset(fp_op, transform=transform)\n",
    "\n",
    "#   data_loader = torch.utils.data.DataLoader(dataset=train_ds,\n",
    "#                                             batch_size=batch_size,\n",
    "#                                             shuffle=True)\n",
    "  print(mnist)\n",
    "  print(train_ds)\n",
    "  D = Discriminator(image_size, hidden_size).to(device)\n",
    "  G = Generator(latent_size, hidden_size, image_size).to(device)\n",
    "\n",
    "  criterion = nn.BCELoss()\n",
    "  d_optimizer = torch.optim.Adam(D.parameters(), lr=0.0002)\n",
    "  g_optimizer = torch.optim.Adam(G.parameters(), lr=0.0002)\n",
    "\n",
    "  # total_step = len(data_loader)\n",
    "  def reset_grad():\n",
    "    d_optimizer.zero_grad()\n",
    "    g_optimizer.zero_grad()\n",
    "  \n",
    "  def rand_z():\n",
    "    return torch.randn(batch_size, latent_size).to(device)\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "      for a in train_dl:\n",
    "          ba_si = a.size(0)\n",
    "\n",
    "          # real image\n",
    "          real_a = a.to(device)\n",
    "\n",
    "          # patch label\n",
    "          real_label = torch.ones(ba_si, *patch, requires_grad=False).to(device)\n",
    "          fake_label = torch.zeros(ba_si, *patch, requires_grad=False).to(device)\n",
    "\n",
    "          # generator\n",
    "          model_gen.zero_grad()\n",
    "\n",
    "          fake_b = model_gen(real_a) # 가짜 이미지 생성\n",
    "          out_dis = model_dis(fake_b, real_b) # 가짜 이미지 식별\n",
    "\n",
    "          gen_loss = loss_func_gan(out_dis, real_label)\n",
    "          pixel_loss = loss_func_pix(fake_b, real_b)\n",
    "\n",
    "          g_loss = gen_loss + lambda_pixel * pixel_loss\n",
    "          g_loss.backward()\n",
    "          opt_gen.step()\n",
    "\n",
    "          # discriminator\n",
    "          model_dis.zero_grad()\n",
    "\n",
    "          out_dis = model_dis(real_b, real_a) # 진짜 이미지 식별\n",
    "          real_loss = loss_func_gan(out_dis,real_label)\n",
    "        \n",
    "          out_dis = model_dis(fake_b.detach(), real_a) # 가짜 이미지 식별\n",
    "          fake_loss = loss_func_gan(out_dis,fake_label)\n",
    "\n",
    "          d_loss = (real_loss + fake_loss) / 2.\n",
    "          d_loss.backward()\n",
    "          opt_dis.step()\n",
    "\n",
    "          loss_hist['gen'].append(g_loss.item())\n",
    "          loss_hist['dis'].append(d_loss.item())\n",
    "\n",
    "          batch_count += 1\n",
    "          if batch_count % 100 == 0:\n",
    "              print('Epoch: %.0f, G_Loss: %.6f, D_Loss: %.6f, time: %.2f min' %(epoch, g_loss.item(), d_loss.item(), (time.time()-start_time)/60))\n",
    "\n",
    "#   for epoch in range(num_epochs):\n",
    "#     fake_images = None\n",
    "#     for i, (images, _) in enumerate(data_loader):\n",
    "#       images = images.view(batch_size, -1).to(device)\n",
    "\n",
    "#       real_label = torch.ones(batch_size, 1).to(device)\n",
    "#       fake_label = torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "#       # ---- D ----\n",
    "#       z = rand_z()\n",
    "\n",
    "#       # real\n",
    "#       real_outputs = D(images)\n",
    "#       d_loss_real = binary_cross_entropy(real_outputs, real_label)\n",
    "#       real_score = real_outputs\n",
    "\n",
    "#       # fake\n",
    "#       fake_images = G(z)\n",
    "#       fake_outputs = D(fake_images)\n",
    "#       d_loss_fake = binary_cross_entropy(fake_outputs, fake_label)\n",
    "#       fake_score = fake_outputs\n",
    "      \n",
    "#       # loss\n",
    "#       d_loss = d_loss_real + d_loss_fake\n",
    "      \n",
    "#       # backprop\n",
    "#       reset_grad()\n",
    "#       d_loss.backward()\n",
    "#       d_optimizer.step()\n",
    "\n",
    "#       # ---- G ----\n",
    "#       z = rand_z()\n",
    "#       fake_images = G(z)\n",
    "#       outputs = D(fake_images)\n",
    "#       g_loss = binary_cross_entropy(outputs, real_label)\n",
    "      \n",
    "#       reset_grad()\n",
    "#       g_loss.backward()\n",
    "#       g_optimizer.step()\n",
    "\n",
    "#       if (i + 1) % 300 == 0:\n",
    "#         print(f'Epoch [{epoch}/{num_epochs}], Step [{i+1}/{total_step}],' + \n",
    "#         f'd_loss: {d_loss:.8f}, g_loss: {g_loss:.8f}, ' + \n",
    "#         f'D(x): {real_score.mean():.2f}, D(G(z)): {fake_score.mean():.2f}')\n",
    "#     if (epoch + 1) % 10 == 0:\n",
    "#       save_image(grid_image(fake_images.view(batch_size, 1, 28, 28)),\n",
    "#                   os.path.join(sample_dir, f'real_images_{epoch+1:03d}.png'))\n",
    "    \n",
    "#     if (epoch + 1) % 50 == 0:\n",
    "#       show_image(fake_images.view(batch_size, 1, 28, 28))\n",
    "    \n",
    "      \n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b69e1a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "big-sleep-new",
   "language": "python",
   "name": "big-sleep-new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
